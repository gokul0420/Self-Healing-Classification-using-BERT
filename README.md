# ğŸ› ï¸ Self-Healing Classification DAG using Transformers, PEFT & CLI

This project is a minimal and explainable **text classification system** built with the following:
- âœ… Hugging Face Transformers (DistilBERT)
- âœ… LoRA Fine-tuning using PEFT
- âœ… Confidence-based Self-Healing DAG (fallback logic)
- âœ… Click-powered Command Line Interface
- âœ… IMDb Dataset from Hugging Face
- âœ… Local logging and modular node design

> ğŸš€ This was developed as part of an interview assignment using the required tech stack.

---

## ğŸ“ Folder Structure

```
self_healing_classifier/
â”œâ”€â”€ data/                    # Dataset loading script
â”‚   â””â”€â”€ download_data.py
â”œâ”€â”€ model/                   # Fine-tuning script and model checkpoint
â”‚   â””â”€â”€ finetune.py
â”œâ”€â”€ src/                     # DAG logic and CLI
â”‚   â”œâ”€â”€ cli.py
â”‚   â”œâ”€â”€ dag.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ nodes/
â”‚       â”œâ”€â”€ inference_node.py
â”‚       â”œâ”€â”€ confidence_check.py
â”‚       â”œâ”€â”€ fallback_node.py
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ logs/                    # Auto-generated logs
â”œâ”€â”€ requirements.txt         # Required Python packages
â””â”€â”€ README.md               # You're here!
```

---

## ğŸ“¦ 1. Installation

### âœ… Create a virtual environment (optional but recommended)

```bash
python -m venv .venv
.venv\Scripts\activate     # On Windows
```

### âœ… Install requirements

```bash
pip install -r requirements.txt
```

If needed, update pip:

```bash
python -m pip install --upgrade pip
```

## ğŸ“¥ 2. Download Dataset

This script downloads the IMDb dataset from Hugging Face and selects 5,000 samples for quick testing.

```bash
python -m data.download_data
```

You will get:
- `data/train.jsonl`
- `data/test.jsonl`

## ğŸ§  3. Fine-tune the Model using LoRA (PEFT)

This script fine-tunes DistilBERT using LoRA (PEFT) on the IMDb dataset.

```bash
python model/finetune.py --epochs 2 --batch_size 16 --output_dir model/checkpoint
```

After training, you'll see your LoRA-adapted model saved at:

```
model/checkpoint/
```

## ğŸ”„ 4. DAG Design â€” How it Works

The application is structured like a LangGraph-style DAG:

1. **Inference Node**: Uses fine-tuned model for sentiment classification.
2. **Confidence Check Node**: If confidence < threshold (e.g. 0.75), triggers fallback.
3. **Fallback Node**:
   - **Mode `ask`**: Asks user for clarification.
   - **Mode `backup`**: Uses zero-shot classifier (facebook/bart-large-mnli) for fallback.

All decisions are logged in `logs/app.log`.

## ğŸ–¥ï¸ 5. Run the CLI

Use this command to classify reviews through the CLI interface:

```bash
python -m src.cli --threshold 0.75 --fallback ask
```

You can switch fallback modes:

```bash
python -m src.cli --threshold 0.75 --fallback backup
```

Example CLI interaction:

```
User input: The movie was decent but dragged on.
[InferenceNode] Predicted label: Positive | Confidence: 61%
[ConfidenceCheckNode] Confidence too low. Triggering fallbackâ€¦
Could you clarify? Is this a 'positive' or 'negative' example: negative
Final Label: Negative  (confidence â‰ˆ 100%)
```

## ğŸ—‚ï¸ 6. Logs

All outputs, decisions, and fallbacks are saved to:

```bash
logs/app.log
```

This helps track model behavior and fallback triggers.
